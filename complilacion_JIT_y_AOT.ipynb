{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5571058b",
   "metadata": {},
   "source": [
    "# Optimización con Vectorización, Profiling y JIT/AOT en Python\n",
    "Este notebook muestra técnicas de optimización: vectorización con NumPy/PyTorch, profiling, benchmarking, JIT con Numba y AOT (caching y Cython)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9815e617",
   "metadata": {},
   "source": [
    "## Conceptos Clave de Vectorización y Eficiencia Computacional\n",
    "\n",
    "La vectorización no solo simplifica el código, sino que proporciona **mejoras sustanciales en rendimiento** al aprovechar:\n",
    "- **Instrucciones SIMD**: Modernas CPUs incluyen extensiones (SSE, AVX, NEON) que ejecutan la misma operación sobre múltiples datos en un solo ciclo, procesando en paralelo vectores de 4, 8 o más valores.\n",
    "- **Operaciones en C optimizado**: Las funciones universales de NumPy (ufuncs) están escritas en C, evitando el overhead del intérprete Python en cada iteración.\n",
    "- **Localidad de memoria**: Al trabajar con arrays contiguos, se maximizan los “cache hits” de L1/L2/L3, reduciendo dramáticamente los accesos a memoria RAM.\n",
    "- **BLAS/MKL/OpenBLAS multihilo**: Para álgebra lineal pesada (`np.dot`, `np.linalg.inv`), estas bibliotecas distribuyen el cómputo entre varios núcleos, equilibrando carga y minimizando tiempos de espera.\n",
    "- **Broadcasting**: Permite operar sin crear copias innecesarias de arrays, reusando la misma memoria y evitando loops Python implícitos.\n",
    "- **Menos GIL**: Al delegar todo el trabajo en C (o en GPU para PyTorch/TensorFlow), se liberan cuellos de botella asociados al Global Interpreter Lock.\n",
    "- **GPU y Tensor Cores**: En Deep Learning, frameworks vectorizados extienden estos principios al hardware de GPU, usando miles de núcleos paralelos y Tensor Cores para acelerar multiplicaciones de matrices a precisión reducida (FP16/TF32).\n",
    "\n",
    "> **Reflexión**: Al vectorizar, transformas tu código de una serie de bucles Python en una composición de operaciones de alto nivel ejecutadas directamente en hardware especializado, logrando **ganancias de 10× a 100×** en muchas tareas numéricas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ecf7366",
   "metadata": {},
   "source": [
    "## Detalle del Pipeline de Optimización\n",
    "\n",
    "1. **Identificación de cuellos de botella**: Usa `%timeit`, `cProfile` o `line_profiler` para detectar loops costosos.\n",
    "2. **Reemplazo por ufuncs**: Sustituye cada iteración por llamadas a `np.add`, `np.multiply`, `np.dot`, etc.\n",
    "3. **Validación de shapes**: Al usar broadcasting, verifica que las dimensiones sean compatibles para evitar bugs silenciosos.\n",
    "4. **Ajuste de dtype**: Selecciona tipos de datos más ligeros (`float32` vs `float64`) para reducir carga de memoria y acelerar cálculos.\n",
    "5. **Test de rendimiento**: Mide antes y después de cada cambio para asegurar que la vectorización aporta mejoras reales."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68246bcd",
   "metadata": {},
   "source": [
    "## Implementación: Generación de Datos, Modelo y Vectorización\n",
    "A continuación el código completo con docstrings y uso de inferencia vectorizada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc47376",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import time\n",
    "\n",
    "def generar_datos_sinteticos(num_muestras=5000):\n",
    "    \"\"\"\n",
    "    Genera datos sintéticos para clasificación binaria.\n",
    "    \"\"\"\n",
    "    X = np.random.randn(num_muestras, 10)\n",
    "    y = (np.sum(X, axis=1) > 0).astype(int)\n",
    "    return X, y\n",
    "\n",
    "class RedNeuronalSimple(nn.Module):\n",
    "    \"\"\"Red neuronal de dos capas.\"\"\"\n",
    "    def __init__(self, dim_entrada=10, dim_oculta=32, dim_salida=2):\n",
    "        super().__init__()\n",
    "        self.modelo = nn.Sequential(\n",
    "            nn.Linear(dim_entrada, dim_oculta), nn.ReLU(), nn.Linear(dim_oculta, dim_salida)\n",
    "        )\n",
    "    def forward(self, x): return self.modelo(x)\n",
    "\n",
    "def entrenar_modelo(modelo, X_train, y_train, epochs=5, batch_size=128):\n",
    "    \"\"\"Entrena con Adam y CrossEntropyLoss.\"\"\"\n",
    "    optimizer = optim.Adam(modelo.parameters(), lr=1e-3)\n",
    "    criterio = nn.CrossEntropyLoss()\n",
    "    modelo.train()\n",
    "    tensor_X = torch.tensor(X_train, dtype=torch.float32)\n",
    "    tensor_y = torch.tensor(y_train, dtype=torch.long)\n",
    "    dataset = torch.utils.data.TensorDataset(tensor_X, tensor_y)\n",
    "    loader  = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "    for epoch in range(epochs):\n",
    "        pérdida=0\n",
    "        for bx,by in loader:\n",
    "            optimizer.zero_grad(); out=modelo(bx); loss=criterio(out, by)\n",
    "            loss.backward(); optimizer.step(); pérdida+=loss.item()\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Pérdida Promedio: {pérdida/len(loader):.4f}\")\n",
    "\n",
    "def inferencia_vectorizada(modelo, X_data, batch_size=None, device='cpu'):\n",
    "    \"\"\"Inferencia en lotes grandes sin hilos manuales.\"\"\"\n",
    "    modelo.to(device).eval()\n",
    "    with torch.no_grad():\n",
    "        tX = torch.tensor(X_data, dtype=torch.float32, device=device)\n",
    "        if batch_size:\n",
    "            preds=[]\n",
    "            for i in range(0, len(tX), batch_size): preds.append(\n",
    "                modelo(tX[i:i+batch_size]).argmax(dim=1).cpu()\n",
    "            )\n",
    "            return torch.cat(preds).numpy()\n",
    "        out = modelo(tX)\n",
    "        return out.argmax(dim=1).cpu().numpy()\n",
    "\n",
    "def main():\n",
    "    X,y=generar_datos_sinteticos(5000)\n",
    "    s=int(0.8*len(X)); X_tr,X_te=X[:s],X[s:]; y_tr,y_te=y[:s],y[s:]\n",
    "    mod=RedNeuronalSimple()\n",
    "    print(\"Entrenando...\"); t0=time.time(); entrenar_modelo(mod,X_tr,y_tr)\n",
    "    print(f\"Entrenamiento en {time.time()-t0:.2f}s\")\n",
    "    print(\"Inferencia...\"); t1=time.time(); p=inferencia_vectorizada(mod,X_te)\n",
    "    print(f\"Inferencia en {time.time()-t1:.2f}s\")\n",
    "    print(f\"Precisión: {(p==y_te).mean()*100:.2f}%\")\n",
    "\n",
    "if __name__=='__main__': main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a941ecc5",
   "metadata": {},
   "source": [
    "## Benchmarking con `time.perf_counter`\n",
    "Medimos tiempos de entrenamiento e inferencia en ejecuciones rápidas de prueba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee41078f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "# Setup rápido\n",
    "X,y = generar_datos_sinteticos(5000)\n",
    "s=int(0.8*len(X)); Xt, Xv = X[:s], X[s:]; yt, yv = y[:s], y[s:]\n",
    "mod = RedNeuronalSimple()\n",
    "# Entrenamiento 1 epoch\n",
    "t0=time.perf_counter(); entrenar_modelo(mod, Xt, yt, epochs=1); print('Train:',time.perf_counter()-t0)\n",
    "# Inferencia completa\n",
    "t1=time.perf_counter(); inferencia_vectorizada(mod, Xv); print('Infer:',time.perf_counter()-t1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d2572b9",
   "metadata": {},
   "source": [
    "## Perfilado con `cProfile`\n",
    "Detectamos funciones con mayor tiempo de ejecución."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "699fda4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cProfile, pstats\n",
    "prof = cProfile.Profile()\n",
    "prof.enable()\n",
    "main()\n",
    "prof.disable()\n",
    "pstats.Stats(prof).sort_stats('cumtime').print_stats(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c9057be",
   "metadata": {},
   "source": [
    "## Análisis línea a línea con `line_profiler`\n",
    "1. Instalar: `pip install line_profiler`\n",
    "2. Decorar funciones críticas con `@profile`\n",
    "3. Ejecutar: `kernprof -l -v vectorizacion_completo.ipynb`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0848d36",
   "metadata": {},
   "source": [
    "## Just-In-Time (JIT) con Numba\n",
    "Con Numba podemos compilar funciones de Python a código máquina en tiempo de ejecución, logrando aceleraciones muy significativas en bucles Python:\n",
    "- `@njit`: compila en modo nopython, sin dependencia del intérprete.\n",
    "- Primera llamada incurre en overhead de compilación; posteriores, el código compilado se ejecuta rápido.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2eb2c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import njit\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "@njit\n",
    "def calc_distances_numba(points, center):\n",
    "    n = points.shape[0]\n",
    "    out = np.empty(n, dtype=np.float64)\n",
    "    for i in range(n):\n",
    "        diff0 = points[i, 0] - center[0]\n",
    "        diff1 = points[i, 1] - center[1]\n",
    "        diff2 = points[i, 2] - center[2]\n",
    "        out[i] = (diff0*diff0 + diff1*diff1 + diff2*diff2) ** 0.5\n",
    "    return out\n",
    "\n",
    "# Dataset de prueba\n",
    "points = np.random.rand(100000, 3)\n",
    "center = np.array([0.5, 0.5, 0.5])\n",
    "\n",
    "# Calentamiento inicial (compilación)\n",
    "calc_distances_numba(points[:1000], center)\n",
    "\n",
    "# Medición de rendimiento\n",
    "t0 = time.perf_counter()\n",
    "dists_numba = calc_distances_numba(points, center)\n",
    "t1 = time.perf_counter()\n",
    "print(f\"Numba JIT tiempo: {t1 - t0:.4f} s\")\n",
    "\n",
    "# Vectorizado NumPy para comparar\n",
    "t2 = time.perf_counter()\n",
    "dists_np = np.sqrt(((points - center) ** 2).sum(axis=1))\n",
    "t3 = time.perf_counter()\n",
    "print(f\"NumPy vectorizado tiempo: {t3 - t2:.4f} s\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "598bb551",
   "metadata": {},
   "source": [
    "## Ahead-of-Time (AOT) y Compilación Persistente\n",
    "- **Cache de Numba**: usando `@njit(cache=True)` almacenamos el binario compilado en disco,\n",
    "  reduciendo el overhead en ejecuciones futuras.\n",
    "- **Cython**: herramienta clásica para compilación AOT, transformando código Python tipado en C.\n",
    "  Requiere compilación previa, pero ofrece máximo rendimiento en secciones críticas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0782f4e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import njit\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "@njit(cache=True)\n",
    "def calc_distances_numba_cached(points, center):\n",
    "    n = points.shape[0]\n",
    "    out = np.empty(n, dtype=np.float64)\n",
    "    for i in range(n):\n",
    "        diff0 = points[i, 0] - center[0]\n",
    "        diff1 = points[i, 1] - center[1]\n",
    "        diff2 = points[i, 2] - center[2]\n",
    "        out[i] = (diff0*diff0 + diff1*diff1 + diff2*diff2) ** 0.5\n",
    "    return out\n",
    "\n",
    "# Ejecución para demostrar cache\n",
    "points = np.random.rand(100000, 3)\n",
    "center = np.array([0.5, 0.5, 0.5])\n",
    "print(\"Compilación y primera ejecución (incluye compilación)...\")\n",
    "calc_distances_numba_cached(points[:1000], center)\n",
    "print(\"Ejecución posterior recupera compilado sin overhead significativo\")\n",
    "t0 = time.perf_counter()\n",
    "calc_distances_numba_cached(points, center)\n",
    "print(f\"Numba JIT con cache tiempo: {time.perf_counter() - t0:.4f} s\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
